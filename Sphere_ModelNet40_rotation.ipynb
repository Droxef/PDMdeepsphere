{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSphere using ModelNet40 dataset\n",
    "### Benchmark with Cohen method S2CNN[[1]](http://arxiv.org/abs/1801.10130) and Esteves method[[2]](http://arxiv.org/abs/1711.06721) and others spherical CNNs\n",
    "Multi-class classification of 3D objects, using the interesting property of rotation equivariance.\n",
    "\n",
    "The 3D objects are projected on a unit sphere.\n",
    "Cohen and Esteves use equiangular sampling, while our method use a HEAlpix sampling\n",
    "\n",
    "Several features are collected:\n",
    "* projection ray length (from sphere border to intersection [0, 2])\n",
    "* cos/sin with surface normal\n",
    "* same features using the convex hull of the 3D object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Load libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # change to chosen GPU to use, nothing if work on CPU\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import healpy as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepsphere import models, experiment_helper, plot, utils\n",
    "from deepsphere.data import LabeledDatasetWithNoise, LabeledDataset\n",
    "import hyperparameters\n",
    "\n",
    "from ModelNet40.load_MN40 import plot_healpix_projection, ModelNet40DatasetTF, ModelNet40DatasetCache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nside = 32\n",
    "datapath = '../data/ModelNet40/' # localisation of the .OFF files\n",
    "exp = 'rot'\n",
    "proc_path = datapath[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = 3        # number of element per file (1 = no augmentation of dataset)\n",
    "nfeat = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_healpix_projection(datapath+'airplane'+\"/train/\"+'airplane'+\"_0069.off\", 32, rotp = True, rot = (90,0,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_rot_dataset = ModelNet40DatasetCache(datapath, 'train', nside=Nside, nfeat=nfeat, augmentation=3, nfile=None, \n",
    "                                           experiment='deepsphere_rot_notr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_rot_tr_dataset = ModelNet40DatasetCache(datapath, 'train', nside=Nside, nfeat=nfeat, augmentation=3, nfile=None, \n",
    "                                              experiment='deepsphere_rot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tr_dataset = ModelNet40DatasetCache(datapath, 'train', nside=Nside, nfeat=nfeat, augmentation=3, nfile=None, \n",
    "                                          experiment='deepsphere')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ModelNet40DatasetCache(datapath, 'train', nside=Nside, nfeat=nfeat, augmentation=1, nfile=None, \n",
    "                                       experiment='deepsphere_notr')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tr_dataset = ModelNet40DatasetCache(datapath, 'test', nside=Nside, nfeat=nfeat, augmentation=3, nfile=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ModelNet40DatasetCache(datapath, 'test', nside=Nside, nfeat=nfeat, augmentation=1, nfile=None,\n",
    "                                        experiment='deepsphere_notr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rot_tr_dataset = ModelNet40DatasetCache(datapath, 'test', nside=Nside, \n",
    "                                       nfeat=nfeat, experiment='deepsphere_rot', augmentation=3, nfile=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rot_dataset = ModelNet40DatasetCache(datapath, 'test', nside=Nside, \n",
    "                                       nfeat=nfeat, experiment='deepsphere_rot_notr', augmentation=3, nfile=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow pipeline datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_TFDataset = ModelNet40DatasetTF(datapath, 'train', nside=Nside,\n",
    "                                      nfeat=nfeat, augmentation=augmentation, nfile=None, \n",
    "                                      experiment='deepsphere'+('_rot' if exp=='rot' else '')+('_notr' if 'pert' not in exp else ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ModelNet40.load_MN40 import compute_mean_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_mean_std(test_rot_dataset, 'test', datapath, Nside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data, phi=None, theta=None):\n",
    "    batch_size, npix, nfeat = data.shape\n",
    "    if theta is None or phi is None:\n",
    "        phi = np.random.rand() * 2 * np.pi\n",
    "        theta = np.random.rand() * np.pi\n",
    "    nside = hp.npix2nside(npix)\n",
    "\n",
    "    # Get theta, phi for non-rotated map\n",
    "    t,p = hp.pix2ang(nside, np.arange(npix), nest=True) #theta, phi\n",
    "\n",
    "    # Define a rotator\n",
    "    r = hp.Rotator(deg=False, rot=[phi, theta])\n",
    "\n",
    "    # Get theta, phi under rotated co-ordinates\n",
    "    trot, prot = r(t,p)\n",
    "\n",
    "    # Interpolate map onto these co-ordinates\n",
    "    new_data = np.zeros(data.shape)\n",
    "    for b in range(batch_size):\n",
    "        for f in range(nfeat):\n",
    "            new_data[b,:,f] = hp.get_interp_val(data[b,:,f], trot, prot, nest=True)\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_equator(data):\n",
    "    return transform(data, 0, np.pi/2).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_shift(data):\n",
    "    batch_size, npix, nfeat = data.shape\n",
    "    new_data = data.copy()\n",
    "    nside = hp.npix2nside(npix)\n",
    "    theta, _ = hp.pix2ang(nside, range(npix))\n",
    "    theta_u = np.unique(theta)\n",
    "    for b in range(batch_size):\n",
    "        for f in range(nfeat):\n",
    "            new_data[b, :, f] = hp.reorder(data[b, :, f], n2r=True)\n",
    "            for t in theta_u:\n",
    "                ligne_ind = np.where(theta==t)[0]\n",
    "                ligne_ind_roll = np.roll(ligne_ind, len(ligne_ind)//4)\n",
    "                new_data[b,ligne_ind_roll,f] = new_data[b,ligne_ind,f]\n",
    "            new_data[b, :, f] = hp.reorder(new_data[b, :, f], r2n = True)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.orthview(im1[:,0], rot=(0,0,0), title=test_rot_dataset.classes[label[0]], nest=True, cmap=cm, min=cmin, max=cmax)\n",
    "plt.figure()\n",
    "im2 = transform_shift(im1[np.newaxis,:,:])\n",
    "hp.orthview(im2[0,:,0], rot=(0,0,0), title=test_rot_dataset.classes[label[0]], nest=True, cmap=cm, min=cmin, max=cmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "steps = train_S2_dataset.N // 1 + 1\n",
    "data_iter = train_S2_dataset.iter(1)\n",
    "cm = plt.cm.RdBu_r\n",
    "cm.set_under('w')\n",
    "for i in tqdm(range(steps)):\n",
    "    data, label = next(data_iter)\n",
    "    im1 = data[0,:,0]\n",
    "    if np.std(im1)>4:\n",
    "        print(np.std(im1))\n",
    "        cmin = np.nanmin(im1)\n",
    "        cmax = np.nanmax(im1)\n",
    "        hp.orthview(im1, rot=(0,0,0), title=train_dataset.classes[label[0]], nest=True, cmap=cm, min=cmin, max=cmax)\n",
    "        plt.figure()\n",
    "#     if i > 1000:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nclass = train_TFDataset.nclass\n",
    "num_elem = train_TFDataset.N\n",
    "print('number of class:',nclass,'\\nnumber of elements:',num_elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Classification using DeepSphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = 'MN40_{}_{}feat_{}aug_{}sides'.format(exp, nfeat, augmentation, Nside)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model with hyperparameters chosen.\n",
    "For each experiment, a new EXP_NAME is chosen, and new hyperparameters are stored.\n",
    "All informations are present 'DeepSphere/ModelNet40/resultsmn40.md'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = hyperparameters.get_params_shrec17_optim(train_TFDataset.N, EXP_NAME, Nside, nclass, \n",
    "                                                  nfeat_in=nfeat, architecture=experiment_type)\n",
    "params[\"tf_dataset\"] = train_TFDataset.get_tf_dataset(params[\"batch_size\"])\n",
    "# params[\"std\"] = [0.001, 0.005, 0.0125, 0.05, 0.15, 0.5]\n",
    "# params[\"full\"] = [True]*6\n",
    "model = models.deepsphere(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree('summaries/{}/'.format(EXP_NAME), ignore_errors=True)\n",
    "shutil.rmtree('checkpoints/{}/'.format(EXP_NAME), ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"the number of parameters in the model is: {:,}\".format(model.get_nbr_var()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_validation, loss_validation, loss_training, t_step, t_batch = model.fit(train_TFDataset, \n",
    "                                                                                 test_dataset, \n",
    "                                                                                 use_tf_dataset=True, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.plot_loss(loss_training, loss_validation, t_step, params['eval_frequency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(train_rot_dataset, None, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(train_rot_tr_dataset, None, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(train_dataset, None, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(train_tr_dataset, None, cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset, None, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.set_transform(transform_shift)\n",
    "print(model.evaluate(test_dataset, None, cache=True))\n",
    "test_dataset.set_transform(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_rot_dataset, None, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_rot_tr_dataset, None, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_tr_dataset, None, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_no_dataset.set_transform(transform_equator)\n",
    "print(model.evaluate(test_no_dataset, None, cache=True))\n",
    "test_no_dataset.set_transform(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test = test_rot_dataset.get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, loss = model.predict(test_rot_dataset, None, cache=True)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elem_ = np.arange(len(labels_test))%3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = test_rot_no_dataset.files[labels_test != predictions]\n",
    "class_pred = np.asarray(test_rot_no_dataset.classes)[predictions[labels_test != predictions].astype(int)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, file in enumerate(files[-10:]):\n",
    "    suffix = os.path.splitext(os.path.split(file)[-1])[0]\n",
    "    data = test_rot_dataset.cache_npy(file, pick_randomly=False, \n",
    "                                         repeat=test_rot_no_dataset.repeat, experiment=test_rot_no_dataset.experiment)\n",
    "    data2 = test_dataset.cache_npy(file, pick_randomly=False, \n",
    "                                         repeat=test_dataset.repeat, experiment=test_dataset.experiment)\n",
    "    im1 = data[elem_[i]]\n",
    "    im2 = data2[0]\n",
    "#     print(np.std(im1))\n",
    "#     print(np.std(im2))\n",
    "#     print(class_pred[i])\n",
    "    cmin = 0\n",
    "    cmax = 1\n",
    "    hp.orthview(im1[:,0], rot=(0,0,0), title=suffix, nest=True, cmap=cm, min=cmin, max=cmax)\n",
    "    plt.figure()\n",
    "    hp.orthview(im2[:,0], rot=(0,0,0), title=suffix, nest=True, cmap=cm, min=cmin, max=cmax)\n",
    "    plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why not working?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _print_histogram(nclass, labels_train, labels_min=None, ylim=None):\n",
    "    if labels_train is None:\n",
    "        return\n",
    "    import matplotlib.pyplot as plt\n",
    "    from collections import Counter\n",
    "    hist_train=Counter(labels_train)\n",
    "    if labels_min is not None:\n",
    "        hist_min = Counter(labels_min)\n",
    "        hist_temp = hist_train - hist_min\n",
    "        hist_min = hist_min - hist_train\n",
    "        hist_train = hist_temp + hist_min\n",
    "#         for i in range(self.nclass):\n",
    "#             hist_train.append(np.sum(labels_train == i))\n",
    "    labels, values = zip(*hist_train.items())\n",
    "    indexes = np.asarray(labels)\n",
    "#     miss = set(indexes) - set(labels)\n",
    "#     if len(miss) is not 0:\n",
    "#         hist_train.update({elem:0 for elem in miss})\n",
    "#     labels, values = zip(*hist_train.items())\n",
    "    width = 1\n",
    "    plt.bar(labels, values, width)\n",
    "    plt.title(\"labels distribution\")\n",
    "    plt.ylim(0,ylim)\n",
    "    #plt.xticks(indexes + width * 0.5, labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_print_histogram(40, labels_test)\n",
    "_print_histogram(40, predictions)\n",
    "_print_histogram(40, labels_test, predictions, ylim=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.spy(confusion_matrix(labels_test, predictions, range(40)), cmap = plt.cm.gist_heat_r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
