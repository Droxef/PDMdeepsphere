{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [DeepSphere]: a spherical convolutional neural network\n",
    "[DeepSphere]: https://github.com/SwissDataScienceCenter/DeepSphere\n",
    "\n",
    "[Nathanaël Perraudin](https://perraudin.info), [Michaël Defferrard](http://deff.ch), Tomasz Kacprzak, Raphael Sgier\n",
    "\n",
    "# Demo: part of sphere classification\n",
    "\n",
    "This demo uses the whole datataset, smoothing, and the addition of noise.\n",
    "\n",
    "**You need a private dataset to execute this notebook.**\n",
    "See the [README](https://github.com/SwissDataScienceCenter/DeepSphere/tree/master#reproducing-the-results-of-the-paper).\n",
    "But you can use it with your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Run on first GPU.\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# To get the CUDA profiler (do it on the CLI before starting jupyter):\n",
    "# export LD_LIBRARY_PATH=/usr/local/cuda-9.0/extras/CUPTI/lib64\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from deepsphere import models, experiment_helper, plot, utils\n",
    "from deepsphere.data import LabeledDatasetWithNoise, LabeledDataset\n",
    "import hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (17, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Definition of the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A) Non tunable parameters\n",
    "These parameters are fixed or the preprocessing script has to be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nside = 1024\n",
    "sigma = 3\n",
    "data_path = '../../../lts2/mdeff/share-michael-nati/data/same_psd/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B) Tunable parameters\n",
    "These parameters can be changed.\n",
    "\n",
    "We choose to work in the noiseless setting by setting `sigma_noise = 0`. This allows this notebook to run an acceptable time. In the noisy case, the training of the network needs considerably more iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 2  # 1,2,4,8 correspond to 12,48,192,768 parts of the sphere.\n",
    "sigma_noise = 2  # Amount of noise for the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data download\n",
    "Set `download` to `True` to download the dataset from zenodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download = False\n",
    "if download:\n",
    "    %run -i 'download.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data preprocessing\n",
    "Apply the preprocessing steps.\n",
    "1. Remove the mean of the maps\n",
    "2. Smooth with a radius of 3 arcmin. (`sigma` parameter)\n",
    "\n",
    "Set `preprocess` to `True` to execute the preprocessing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = False\n",
    "if preprocess:\n",
    "    %run -i 'data_preprocess.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us display the resulting PSDs of the preprocessed data. We pre-computed the PSDs for faster execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute = False\n",
    "if compute:\n",
    "    psd = experiment_helper.psd\n",
    "    data_path = 'data/same_psd/'\n",
    "    ds1 = np.load(data_path+'smoothed_class1_sigma{}.npz'.format(sigma))['arr_0']\n",
    "    ds2 = np.load(data_path+'smoothed_class2_sigma{}.npz'.format(sigma))['arr_0']\n",
    "    psds_img1 = [psd(img) for img in ds1]\n",
    "    psds_img2 = [psd(img) for img in ds2]\n",
    "    np.savez('results/psd_data_sigma{}'.format(sigma), psd_class1=psds_img1, psd_class2=psds_img2)\n",
    "else:\n",
    "    psds_img1 = np.load('results/psd_data_sigma{}.npz'.format(sigma))['psd_class1']\n",
    "    psds_img2 = np.load('results/psd_data_sigma{}.npz'.format(sigma))['psd_class2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PSD of the two classes is almost indistinguishable. \n",
    "\n",
    "Spoiler Alert! This is the reason why PSD features are not good enough to classify the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ell = np.arange(psds_img1.shape[1])\n",
    "\n",
    "plot.plot_with_std(ell,np.stack(psds_img1)*ell*(ell+1), label='class 1, $\\Omega_m=0.31$, $\\sigma_8=0.82$, $h=0.7$', color='r')\n",
    "plot.plot_with_std(ell,np.stack(psds_img2)*ell*(ell+1), label='class 2, $\\Omega_m=0.26$, $\\sigma_8=0.91$, $h=0.7$', color='b')\n",
    "plt.legend(fontsize=16);\n",
    "plt.xlim([11, np.max(ell)])\n",
    "plt.ylim([1e-6, 5e-4])\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('$\\ell$: spherical harmonic index', fontsize=18)\n",
    "plt.ylabel('$C_\\ell \\cdot \\ell \\cdot (\\ell+1)$', fontsize=18)\n",
    "plt.title('Power Spectrum Density, 3-arcmin smoothing, noiseless, Nside=1024', fontsize=18);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data loading\n",
    "The following functions will\n",
    "1. Load the preprocessed data\n",
    "2. Create samples by dividing the complete spheres in patches (based on healpix sampling). See the function `hp_split` of `experiment_helper.py` for more specific informations.\n",
    "\n",
    "The function that load the testing data will additionally add the noise to the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_raw_train, labels_raw_train, x_raw_std = experiment_helper.get_training_data(sigma, order, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_raw_test, labels_test, _ = experiment_helper.get_testing_data(sigma, order, sigma_noise, x_raw_std, data_path[:-9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Solve the problem using histogram features and an SVM classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Features compuation and dataset creation\n",
    "The following function prepare the features for the SVM classifier.\n",
    "1. It splits the training data into a training and a validation set.\n",
    "2. It augments the training set by adding different realization of random noise to the sample\n",
    "3. It computes the histogram features for the training, validation and testing set.\n",
    "4. It normalizes the features in order for them to have a mean of 0 and a variance of 1.\n",
    "\n",
    "The features are computed using the function `histogram` of `experiment_helper.py`.\n",
    "\n",
    "We use 10 different noise realization by setting `augmentation=10` in order to increase the number of training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = experiment_helper.data_preprossing(x_raw_train, labels_raw_train, x_raw_test, sigma_noise, feature_type='histogram', augmentation=10)\n",
    "features_train, labels_train, features_validation, labels_validation, features_test = ret "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  Classification using SVM\n",
    "Let us test classify our data using an SVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_train, error_validation, C = experiment_helper.err_svc_linear(features_train, labels_train, features_validation, labels_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The validation error is {}%'.format(error_validation * 100), flush=True)\n",
    "print('The Training error is {}%'.format(error_train * 100), flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute the error on the testing set. To avoid complexity, we do a small mistake that advantage the SVM classifer: we do cross-validation on the testing set.\n",
    "\n",
    "While this is wrong, the spherical CNN still clearly outperform the SVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, error_test = experiment_helper.err_svc_linear_single(C, features_train, labels_train, features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The testing error is {}%'.format(error_test * 100), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Histogram features visualization\n",
    "\n",
    "To get a grasp of what is happening, let us plot the histogram of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmin = np.min(x_raw_train)\n",
    "cmax = np.max(x_raw_train)\n",
    "bins = 100\n",
    "x = np.linspace(cmin,cmax,bins)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "x_hist = experiment_helper.histogram(x_raw_train, cmin, cmax)\n",
    "plot.plot_with_std(x, x_hist[labels_raw_train==0], color='b', label='class 1', ax=axes[0])\n",
    "plot.plot_with_std(x, x_hist[labels_raw_train==1], color='r', label='class 2', ax=axes[0])\n",
    "axes[0].legend()\n",
    "axes[0].set_title('Histogram - Noiselss case');\n",
    "\n",
    "if sigma_noise:\n",
    "    # Updating cmin and cmax does not really affect the features. \n",
    "    # We keep the same as in the noisless case in order to have the same x axis.\n",
    "    x_hist = experiment_helper.histogram(x_raw_train+sigma_noise*np.random.randn(*x_raw_train.shape), cmin, cmax)\n",
    "    plot.plot_with_std(x, x_hist[labels_raw_train==0], color='b', label='class 1', ax=axes[1])\n",
    "    plot.plot_with_std(x, x_hist[labels_raw_train==1], color='r', label='class 2', ax=axes[1])\n",
    "    axes[1].legend()\n",
    "    axes[1].set_title('Histogram-  Noisy case');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These histogram are normalized in order to get the final features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot.plot_with_std(features_train[labels_train==0], color='b', label='class 1')\n",
    "ax = plot.plot_with_std(features_train[labels_train==1], color='r', label='class 2', ax=ax)\n",
    "ax.legend()\n",
    "ax.set_title('Histogram features');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = plot.plot_with_std(features_validation[labels_validation==0,:80], color='b', label='class 1')\n",
    "# ax = plot.plot_with_std(features_validation[labels_validation==1,:80], color='r', label='class 2', ax=ax)\n",
    "# ax.legend()\n",
    "# ax.set_title('Histogram features - Validation set');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = plot.plot_with_std(features_test[labels_test==0,:80], color='b', label='class 1')\n",
    "# ax = plot.plot_with_std(features_test[labels_test==1,:80], color='r', label='class 2', ax=ax)\n",
    "# ax.legend()\n",
    "# ax.set_title('Histogram features - Test set');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Solve the problem using PSD features and an SVM classifier\n",
    "Solving the problem with PSD features is very similar than solving it with histogram features. Hence we are not describing each step.\n",
    "\n",
    "The computation of the PSD features is actually very expensive. Since the classifier will also fail miserably, you may just want to not exectute this part of the notebook. In order to reduce the amount of PSD to be computed, we disable the dataset augementation by setting `augmentation=1`. Nevertheless, we use augmentation for the results in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = experiment_helper.data_preprossing(x_raw_train, labels_raw_train, x_raw_test, sigma_noise, feature_type='psd', augmentation=1)\n",
    "features_train, labels_train, features_validation, labels_validation, features_test = ret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_train, error_validation, C = experiment_helper.err_svc_linear(features_train, labels_train, features_validation, labels_validation)\n",
    "print('The validation error is {}%'.format(error_validation * 100), flush=True)\n",
    "print('The Training error is {}%'.format(error_train * 100), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, error_test = experiment_helper.err_svc_linear_single(C, features_train, labels_train, features_test, labels_test)\n",
    "print('The testing error is {}%'.format(error_test * 100), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 PSD features visualization\n",
    "\n",
    "To get a grasp of what is happening, let us plot the psd features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ell = np.arange(features_train.shape[1])\n",
    "ax = plot.plot_with_std(ell, features_train[labels_train==0], color='b', label='class 1')\n",
    "ax = plot.plot_with_std(ell, features_train[labels_train==1], color='r', label='class 2', ax=ax)\n",
    "ax.legend()\n",
    "ax.set_title('PSD features');\n",
    "# plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ell = np.arange(features_train.shape[1])\n",
    "# ax = plot.plot_with_std(ell, features_validation[labels_validation==0], color='b', label='class 1')\n",
    "# ax = plot.plot_with_std(ell, features_validation[labels_validation==1], color='r', label='class 2', ax=ax)\n",
    "# ax.legend()\n",
    "# ax.set_title('PSD features - validation dataset');\n",
    "# # plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ell = np.arange(features_train.shape[1])\n",
    "# ax = plot.plot_with_std(ell, features_test[labels_test==0], color='b', label='class 1')\n",
    "# ax = plot.plot_with_std(ell, features_test[labels_test==1], color='r', label='class 2', ax=ax)\n",
    "# ax.legend()\n",
    "# ax.set_title('PSD features - testing dataset');\n",
    "# # plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Classification using Deep Sphere\n",
    "\n",
    "Let us now classify our data using a spherical convolutional neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Preparation of the dataset\n",
    "Let us create the datafor the spherical neural network. It is simply the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = experiment_helper.data_preprossing(x_raw_train, labels_raw_train, x_raw_test, sigma_noise, feature_type=None, train_size=0.8)\n",
    "features_train, labels_train, features_validation, labels_validation, features_test = ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spherical neural network will uses a Dataset object that need to be initialized. The object `LabeledDatasetWithNoise` will add noise to the raw data at the time of training. It will slowly increase the amount of noise during `nit` iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = LabeledDatasetWithNoise(features_train, labels_train, end_level=sigma_noise)\n",
    "validation = LabeledDataset(features_validation, labels_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Building the Network\n",
    "\n",
    "We now create our spherical neural network. We use one architecture, a fully convolutional architecture (see the exact parameters in `hyperparameters.py`), for all the problems (that is for all configurations of `order` and `sigma_noise`. A smaller `order` means more pixels per sample, that is more data for a prediction. It translates to higher accuracy as the network is more confident about its prediction (as they are averaged across spatial locations).\n",
    "\n",
    "For the paper, we selected a conservative set of parameters that were providing good results across the board. To train faster, diminish `num_epochs`, or interrupt training whenever you get bored. To reproduce all the results from the paper, the easiest is to run the `experiments_deepsphere.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntype = 'FCN'\n",
    "EXP_NAME = '40sim_{}sides_{:0.1f}noise_{}order_{}sigma_{}'.format(Nside, sigma_noise, order, sigma, ntype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = hyperparameters.get_params(training.N, EXP_NAME, order, Nside, ntype)\n",
    "#params['profile'] = True  # See computation time and memory usage in Tensorboard.\n",
    "#params['debug'] = True  # Debug the model in Tensorboard.\n",
    "model = models.deepsphere(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup before running again.\n",
    "shutil.rmtree('summaries/{}/'.format(EXP_NAME), ignore_errors=True)\n",
    "shutil.rmtree('checkpoints/{}/'.format(EXP_NAME), ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Find an optimal learning rate (optional)\n",
    "\n",
    "The learning rate is the most important hyper-parameter. A technique to find an optimal value is to visualize the validation loss while increasing the learning rate. One way to define the optimal learning rate is to search for the largest value looking for which the validation loss still decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup = params.copy()\n",
    "# \n",
    "# params, learning_rate = utils.test_learning_rates(params, training.N, 1e-6, 1e-1, num_epochs=20)\n",
    "# \n",
    "# shutil.rmtree('summaries/{}/'.format(params['dir_name']), ignore_errors=True)\n",
    "# shutil.rmtree('checkpoints/{}/'.format(params['dir_name']), ignore_errors=True)\n",
    "# \n",
    "# model = models.deepsphere(**params)\n",
    "# _, loss_validation, _, _ = model.fit(training, validation)\n",
    "# \n",
    "# params.update(backup)\n",
    "#\n",
    "# plt.semilogx(learning_rate, loss_validation, '.-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Training the network\n",
    "\n",
    "Here are a few remarks.\n",
    "* The model will create tensorboard summaries in the `summaries` folder. Start tensorboard with `cd summaries` then `tensorboard --logdir .`, and open <http://localhost:6006> in a browser tab to visualize training progress and statistics about the learned parameters. You can debug the model by setting `params['debug'] = True` and launching tensorboard with `tensorboard --logdir . --debugger_port 6064`.\n",
    "* You probably need a GPU to train the model in an acceptable amount of time.\n",
    "* You will get slightly different results every time the network is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_validation, loss_validation, loss_training, t_step = model.fit(training, validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see below that the classifier does not overfit the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.plot_loss(loss_training, loss_validation, t_step, params['eval_frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_validation = np.expand_dims(features_validation,2)\n",
    "error_validation = experiment_helper.model_error(model, features_validation, labels_validation)\n",
    "print('The validation error is {:.2%}'.format(error_validation), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = np.expand_dims(features_test,2)\n",
    "error_test = experiment_helper.model_error(model, features_test, labels_test)\n",
    "print('The testing error is {:.2%}'.format(error_test), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Filters visualization\n",
    "\n",
    "The package offers a few different visualizations for the learned filters. First we can simply look at the Chebyshef coefficients. This visualization is not very interpretable for human, but can help for debugging problems related to optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer=2\n",
    "ind_in = range(6) # Should be None if layer=1\n",
    "ind_out = range(4)\n",
    "model.plot_chebyshev_coeffs(layer, ind_in, ind_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe the Chebyshef polynomials, i.e the filters in the graph spectral domain. This visuallization can help to understand wich graph frequencies are picked by the filtering operation. It mostly interpretable by the people for the graph signal processing community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_filters_spectral(layer, ind_in, ind_out);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes one of the most human friendly representation of the filters. It consists the section of the filters \"projected\" on the sphere. Because of the irregularity of the healpix sampling, this representation of the filters may not look very smooth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "model.plot_filters_section(layer, ind_in, ind_out, title='');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually, we can simply look at the filters on sphere. This representation clearly displays the sampling artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10, 17)\n",
    "model.plot_filters_gnomonic(layer, ind_in, ind_out, title='');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
